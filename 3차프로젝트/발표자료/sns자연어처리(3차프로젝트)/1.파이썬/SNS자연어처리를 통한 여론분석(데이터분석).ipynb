{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "\n",
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install JPype1-1.1.2-cp38-cp38-win_amd64.whl\n",
    "\n",
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd \n",
    "import re\n",
    "import matplotlib.pyplot  as plt\n",
    "from konlpy.tag import Kkma\n",
    "import konlpy\n",
    "kkma = Kkma(max_heap_size= 1024 * 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_list = [1,2,3,4,5,7,8,9,10,11,12,13,14,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in call_list:\n",
    "    globals()['globals_{}'.format(i)] = pd.read_csv('./tw크롤링 데이터/snsresultsampleAPI'+str(i)+'.csv', encoding = 'utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#중복제거\n",
    "for i in call_list:\n",
    "    globals()['sample_{}'.format(i)] = globals()['globals_{}'.format(i)].drop_duplicates(['내용'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([\n",
    "    sample_1,sample_2,sample_3,sample_4, sample_5,\n",
    "        sample_7,sample_8,sample_9,sample_10,sample_11,sample_12,\n",
    "                  sample_13,sample_14,sample_15\n",
    "])\n",
    "data.fillna(value = \"FILLVALUE\")\n",
    "\n",
    "data['토큰화']=0\n",
    "data['추출단어']= \"\"\n",
    "data['감성지수']=0\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " ### from konlpy.tag import Kkma\n",
    " kkma = Kkma()\n",
    " \n",
    " kkma.morphs         #형태소 분석\n",
    " \n",
    " kkma.nouns          #명사 분석\n",
    " \n",
    " kkma.pos            #형태소 분석 태깅\n",
    " \n",
    " kkma.sentences      #문장 분석\n",
    " \n",
    " \n",
    " ### 사용예시\n",
    " kkma.morphs(text)\n",
    " \n",
    " '공부', '를', '하', '면', '하', 'ㄹ수록', '모르', '는', '것', '이', '많', 다는', '것', '을', '알', '게', '되', 'ㅂ니다', '.'\n",
    " \n",
    " kkma.nouns(text)\n",
    " \n",
    " '대학', '통계학', '이산', '이산수학', '수학', '등'\n",
    " \n",
    " kkma.pos(text)\n",
    " \n",
    " ('다', 'MAG'), ('까먹', 'VV'), ('어', 'ECD'), ('버리', 'VXV'), ('었', 'EPT'), ('네요', 'EFN'), ('?', 'SF'), ('ㅋㅋ', 'EMO')\n",
    " \n",
    " kkma.sentences(text)\n",
    " \n",
    " '그래도 계속 공부합니다.', '재밌으니까!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okt \n",
    " - Okt(한국어 트위터 형태소 분석기)\n",
    " - kkma처리속도보다 현저히 빠름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt(max_heap_size= 1024 * 6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## okt.nouns(text)로 내용안에 명사를 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errdictionary = []\n",
    "import re\n",
    "num = 0\n",
    "errnum = 0\n",
    "\n",
    "for i in range(len(data.iloc[:,1])):\n",
    "    try:\n",
    "        text = data.iloc[i,1]\n",
    "        \n",
    "        if type(text) != float:\n",
    "            wordlist = okt.nouns(text)\n",
    "            num+=1\n",
    "            strwordlist = \"\"\n",
    "            \n",
    "            for word in wordlist:       \n",
    "                if len(word) != 1:\n",
    "                    strwordlist = strwordlist+\" \"+word\n",
    "            data.iloc[i,3] = strwordlist\n",
    "            \n",
    "        else:\n",
    "            errdictionary.append(num)\n",
    "            \n",
    "        if num%10000 == 0:\n",
    "            print(num/10000,\"만번째 실행중\") \n",
    "        \n",
    "    except:\n",
    "        print(\"data \",num,\"번째에서 오류\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #  단어(명사) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentimentdictionary = []\n",
    "errdictionary = []\n",
    "dict_df = pd.DataFrame(columns=['인덱스','단어'])\n",
    "import re\n",
    "num = 0\n",
    "errnum = 0\n",
    "\n",
    "for text in data.iloc[:,1]:\n",
    "    try:\n",
    "        if type(text) != float:\n",
    "            wordlist = okt.nouns(text)\n",
    "            num+=1\n",
    "        else:\n",
    "            errdictionary.append(num)\n",
    "        if num%10000 == 0:\n",
    "            print(num/10000,\"만번째 실행중\") \n",
    "        \n",
    "        for word in wordlist:\n",
    "\n",
    "            if len(word) != 1:\n",
    "                sentimentdictionary.append(word)\n",
    "    except:\n",
    "        print(\"data \",num,\"번째에서 오류\")  \n",
    "        \n",
    "   \n",
    "data_df = pd.DataFrame(sentimentdictionary, columns = ['단어'])\n",
    "\n",
    "# 모든 특수문자와 숫자 제거\n",
    "data_df[\"단어\"] = data_df[\"단어\"].str.replace(pat=r'[^\\w]', repl=r'', regex=True)\n",
    "data_df[\"단어\"] = data_df[\"단어\"].str.replace('\\d+', '')\n",
    "            \n",
    "#중복제거\n",
    "dict_df = pd.DataFrame(data_df['단어'].unique(), columns = ['단어'])\n",
    "\n",
    "dict_df['빈도수']=0\n",
    "#40만개의 트윗에서 명사로 추정되는 단어 200만개를 추출\n",
    "#200만개의 단어는 총 8만개의 단어로 이루어져있음\n",
    "#dict_df['빈도수']에 단어별 쓰인 횟수를 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 단어 빈도수 체크\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for i in range(len(dict_df['단어'])):\n",
    "    dict_df.iloc[i,1] = len(data_df[data_df['단어'] == dict_df.iloc[i,0]])\n",
    "    num+=1\n",
    "    if num%10000 == 0:\n",
    "        print(num/10000,\"만번째 실행중\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#사전단어를 csv로 저장\n",
    "\n",
    "dict_df.to_csv('dict_of_count.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#빈도수로 정렬 많이 쓰인 단어부터 감성수치 라벨링으로 신뢰도 업데이트후 불러오기\n",
    "scoredict = pd.read_csv('./dict_of_count(sort).csv')\n",
    "\n",
    "scoredict = scoredict.fillna(value = 0)\n",
    "scoredict.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트윗별 감성스코어 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num= 0\n",
    "for i in range(len(data.iloc[:,3])):\n",
    "    try:\n",
    "        #okt.nouns로 추출한 단어 스플릿\n",
    "        vv_word_list = data.iloc[i,3].split()\n",
    "        score = 0\n",
    "        num+=1\n",
    "        if num%10000 == 0:\n",
    "            print(num/10000,\"만번째 실행중\")\n",
    "            \n",
    "        if len(vv_word_list) !=0:\n",
    "            for j in range(len(vv_word_list)):\n",
    "                vv_word = vv_word_list[j]\n",
    "                score_sum_df = scoredict[scoredict['단어'] == vv_word] ## df[조건식]\n",
    "                temp= score_sum_df['라벨링']\n",
    "                #단어마다 감성스코어를 확인후 점수합산\n",
    "                if len(temp) !=0:\n",
    "                    score += int(temp)\n",
    "    except:\n",
    "        \n",
    "        #오류나는 원인 분석하니 트윗중 불용어를 제외하면\n",
    "        #내용이없는 경우(7~8%정도) data['추출단어']가 널값이라 오류났음\n",
    "        #이런것들은 0점처리\n",
    "        print(num,\"번째에서 오류\")\n",
    "        num+=1\n",
    "        score = 0\n",
    "    data.iloc[i,4]= score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data_emo_score.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 감성지수(기업별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "list = data['종목'].unique()\n",
    "result_emo_score = pd.DataFrame(columns=['기업명','감성지수'])\n",
    "for i in range(len(list)):\n",
    "    cop = list[i]\n",
    "    temp_df = data[data['종목']==cop]\n",
    "    score_sum = temp_df['감성지수'].sum()\n",
    "    data_to_insert = {'기업명': cop, '감성지수': score_sum}\n",
    "    result_emo_score = result_emo_score.append(data_to_insert, ignore_index=True)\n",
    "    num += 1\n",
    "    if num%100 == 0:\n",
    "        print(num/100,\"백번째 실행중\")\n",
    "result_emo_score_sort = result_emo_score.sort_values(by=['감성지수'], axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_emo_score_sort.to_csv('result_emo_score_sort.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
